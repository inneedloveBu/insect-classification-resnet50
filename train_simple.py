"""
ç®€åŒ–ä½†å¯é çš„IP102è®­ç»ƒè„šæœ¬
è¿è¡Œ: python train_simple.py
"""
from PIL import Image
import os
import sys
import tensorflow as tf
import numpy as np
import pandas as pd
from datetime import datetime
from sklearn.model_selection import train_test_split

# è®¾ç½®è·¯å¾„å’Œå‚æ•°
DATA_PATH = 'ip102_v1.1'
IMAGES_DIR = os.path.join(DATA_PATH, 'images')
IMG_SIZE = (224, 224)
BATCH_SIZE = 16
NUM_CLASSES = 102
EPOCHS = 5  # å…ˆè®­ç»ƒ5ä¸ªå‘¨æœŸ

def load_annotation_file(filename):
    """åŠ è½½æ ‡æ³¨æ–‡ä»¶"""
    filepath = os.path.join(DATA_PATH, filename)
    data = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                filename = parts[0]
                class_id = int(parts[1])
                filepath_img = os.path.join(IMAGES_DIR, filename)
                if os.path.exists(filepath_img):
                    data.append({'filename': filename, 'class_id': class_id, 'filepath': filepath_img})
    return pd.DataFrame(data)

def create_simple_model():
    """åˆ›å»ºç®€åŒ–æ¨¡å‹"""
    # ä½¿ç”¨é¢„è®­ç»ƒçš„ResNet50V2
    base_model = tf.keras.applications.ResNet50V2(
        include_top=False,
        weights='imagenet',
        input_shape=(224, 224, 3)
    )
    base_model.trainable = False  # å†»ç»“é¢„è®­ç»ƒå±‚
    
    # æ„å»ºå®Œæ•´æ¨¡å‹
    model = tf.keras.Sequential([
        base_model,
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(512, activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')
    ])
    
    # ç¼–è¯‘æ¨¡å‹
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def load_and_preprocess_image(filepath, label):
    """åŠ è½½å’Œé¢„å¤„ç†å•å¼ å›¾ç‰‡"""
    img = tf.io.read_file(filepath)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, IMG_SIZE)
    img = img / 255.0
    return img, label

def main():
    print("=" * 60)
    print("ğŸ“Š IP102æ˜†è™«åˆ†ç±» - ç®€åŒ–è®­ç»ƒè„šæœ¬")
    print("=" * 60)
    
    # 1. åˆ›å»ºä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = f"training_simple_{timestamp}"
    model_dir = os.path.join(save_dir, "models")
    os.makedirs(model_dir, exist_ok=True)
    print(f"âœ… ä¿å­˜ç›®å½•: {save_dir}")
    
    # 2. åŠ è½½æ•°æ®
    print("\n[1/4] åŠ è½½æ•°æ®...")
    try:
        train_df = load_annotation_file('train.txt')
        val_df = load_annotation_file('val.txt')
        test_df = load_annotation_file('test.txt')
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ:")
        print(f"   è®­ç»ƒé›†: {len(train_df)} å¼ å›¾ç‰‡")
        print(f"   éªŒè¯é›†: {len(val_df)} å¼ å›¾ç‰‡")
        print(f"   æµ‹è¯•é›†: {len(test_df)} å¼ å›¾ç‰‡")
        
        # å–å‰1000å¼ ä½œä¸ºå¿«é€Ÿè®­ç»ƒï¼ˆå®Œæ•´è®­ç»ƒå¯å»æ‰è¿™ä¸ªé™åˆ¶ï¼‰
        if len(train_df) > 1000:
            train_df = train_df.sample(1000, random_state=42)
            print(f"   ä½¿ç”¨å‰1000å¼ å›¾ç‰‡è¿›è¡Œå¿«é€Ÿè®­ç»ƒ")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return
    
    # 3. å‡†å¤‡æ•°æ®ç®¡é“
    print("\n[2/4] å‡†å¤‡æ•°æ®ç®¡é“...")
    
    # è½¬æ¢æ•°æ®ä¸ºTensorFlowæ ¼å¼
    train_images = train_df['filepath'].values
    train_labels = tf.keras.utils.to_categorical(train_df['class_id'].values, NUM_CLASSES)
    
    val_images = val_df['filepath'].values[:200]  # åªç”¨200å¼ éªŒè¯
    val_labels = tf.keras.utils.to_categorical(val_df['class_id'].values[:200], NUM_CLASSES)
    
    # åˆ›å»ºTensorFlowæ•°æ®é›†
    def create_dataset(image_paths, labels, shuffle=True):
        dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))
        
        def load_wrapper(filepath, label):
            return load_and_preprocess_image(filepath, label)
        
        dataset = dataset.map(load_wrapper, num_parallel_calls=tf.data.AUTOTUNE)
        
        if shuffle:
            dataset = dataset.shuffle(buffer_size=1000)
        
        dataset = dataset.batch(BATCH_SIZE)
        dataset = dataset.prefetch(tf.data.AUTOTUNE)
        
        return dataset
    
    train_dataset = create_dataset(train_images, train_labels, shuffle=True)
    val_dataset = create_dataset(val_images, val_labels, shuffle=False)
    
    # 4. æ„å»ºæ¨¡å‹
    print("\n[3/4] æ„å»ºæ¨¡å‹...")
    try:
        model = create_simple_model()
        model.summary()
        print("âœ… æ¨¡å‹æ„å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹æ„å»ºå¤±è´¥: {e}")
        return
    
    # 5. è®¾ç½®å›è°ƒå‡½æ•°
    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(
            filepath=os.path.join(model_dir, 'best_model.h5'),
            monitor='val_accuracy',
            save_best_only=True,
            verbose=1
        ),
        tf.keras.callbacks.CSVLogger(
            filename=os.path.join(save_dir, 'training_log.csv')
        ),
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=3,
            restore_best_weights=True,
            verbose=1
        )
    ]
    
    # 6. å¼€å§‹è®­ç»ƒ
    print("\n[4/4] å¼€å§‹è®­ç»ƒ...")
    print(f"   è®­ç»ƒå‘¨æœŸ: {EPOCHS}")
    print(f"   æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"   å¼€å§‹æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
    print("-" * 60)
    
    try:
        history = model.fit(
            train_dataset,
            validation_data=val_dataset,
            epochs=EPOCHS,
            callbacks=callbacks,
            verbose=1
        )
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = os.path.join(model_dir, 'final_model.h5')
        model.save(final_model_path)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆ!")
        print("=" * 60)
        print(f"   æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {history.history['val_accuracy'][-1]:.4f}")
        print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {max(history.history['val_accuracy']):.4f}")
        print(f"   æ¨¡å‹ä¿å­˜ä½ç½®: {model_dir}/")
        print(f"   1. æœ€ä½³æ¨¡å‹: {model_dir}/best_model.h5")
        print(f"   2. æœ€ç»ˆæ¨¡å‹: {final_model_path}")
        print(f"   3. è®­ç»ƒæ—¥å¿—: {save_dir}/training_log.csv")
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()    


    # 7. æ–°å¢ï¼šæµ‹è¯•é›†è¯„ä¼°ï¼ˆæ·»åŠ åœ¨è®­ç»ƒå®Œæˆåï¼‰
    # ##############################################
    print("\n[5/5] æµ‹è¯•é›†è¯„ä¼°...")
    
    try:
        # å‡†å¤‡æµ‹è¯•æ•°æ®ï¼ˆå’ŒéªŒè¯é›†ç±»ä¼¼ï¼‰
        test_images = test_df['filepath'].values[:200]  # ç”¨200å¼ æµ‹è¯•
        test_labels = tf.keras.utils.to_categorical(
            test_df['class_id'].values[:200], 
            NUM_CLASSES
        )
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        test_dataset = create_dataset(test_images, test_labels, shuffle=False)
        
        # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œè¯„ä¼°
        best_model_path = os.path.join(model_dir, 'best_model.h5')
        if os.path.exists(best_model_path):
            best_model = tf.keras.models.load_model(best_model_path)
            
            # è¯„ä¼°æ¨¡å‹
            test_loss, test_accuracy = best_model.evaluate(test_dataset, verbose=1)
            
            print(f"\nğŸ“Š æµ‹è¯•é›†æ€§èƒ½:")
            print(f"   æµ‹è¯•æŸå¤±: {test_loss:.4f}")
            print(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")
            
            # è¯¦ç»†è¯„ä¼°ï¼ˆå¯é€‰ï¼‰
            print("\nğŸ“‹ è¯¦ç»†åˆ†æ:")
            detailed_evaluation(best_model, test_df, num_samples=50)
            
        else:
            print("âš ï¸  æœ€ä½³æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°")
            
    except Exception as e:
        print(f"âš ï¸  æµ‹è¯•è¯„ä¼°å¤±è´¥: {e}")
    






        # 8. ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        plot_training_history(history, save_dir)
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()





def plot_training_history(history, save_dir):
    """ç»˜åˆ¶è®­ç»ƒå†å²"""
    try:
        import matplotlib.pyplot as plt
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # å‡†ç¡®ç‡
        ax1.plot(history.history['accuracy'], 'b-o', label='è®­ç»ƒ', markersize=4)
        ax1.plot(history.history['val_accuracy'], 'r-s', label='éªŒè¯', markersize=4)
        ax1.set_title('æ¨¡å‹å‡†ç¡®ç‡')
        ax1.set_xlabel('è®­ç»ƒå‘¨æœŸ')
        ax1.set_ylabel('å‡†ç¡®ç‡')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æŸå¤±
        ax2.plot(history.history['loss'], 'b-o', label='è®­ç»ƒ', markersize=4)
        ax2.plot(history.history['val_loss'], 'r-s', label='éªŒè¯', markersize=4)
        ax2.set_title('æ¨¡å‹æŸå¤±')
        ax2.set_xlabel('è®­ç»ƒå‘¨æœŸ')
        ax2.set_ylabel('æŸå¤±')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        plot_path = os.path.join(save_dir, 'training_history.png')
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        print(f"   4. è®­ç»ƒæ›²çº¿: {plot_path}")
        
        plt.show()
        
    except Exception as e:
        print(f"âš ï¸  ç»˜åˆ¶å›¾è¡¨å¤±è´¥: {e}")

# æµ‹è¯•ä»£ç  - éªŒè¯æ•°æ®åŠ è½½
def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½æ˜¯å¦æ­£å¸¸"""
    print("ğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½...")
    
    # æµ‹è¯•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(DATA_PATH):
        print(f"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {DATA_PATH}")
        return False
    
    if not os.path.exists(IMAGES_DIR):
        print(f"âŒ å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {IMAGES_DIR}")
        return False
    
    # æµ‹è¯•åŠ è½½ä¸€å¼ å›¾ç‰‡
    try:
        df = load_annotation_file('train.txt')
        if len(df) == 0:
            print("âŒ æ ‡æ³¨æ–‡ä»¶ä¸ºç©º")
            return False
        
        # æµ‹è¯•ç¬¬ä¸€å¼ å›¾ç‰‡
        first_img = df.iloc[0]['filepath']
        if os.path.exists(first_img):
            print(f"âœ… æ•°æ®åŠ è½½æµ‹è¯•é€šè¿‡!")
            print(f"   æ‰¾åˆ° {len(df)} å¼ è®­ç»ƒå›¾ç‰‡")
            print(f"   ç¤ºä¾‹å›¾ç‰‡: {first_img}")
            return True
        else:
            print(f"âŒ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {first_img}")
            return False
            
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        return False

# a) å¢åŠ æµ‹è¯•é›†è¯„ä¼°
def evaluate_model(model, test_df):
    """è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½"""
    test_images = test_df['filepath'].values[:200]  # å–200å¼ æµ‹è¯•
    test_labels = tf.keras.utils.to_categorical(
        test_df['class_id'].values[:200], 
        NUM_CLASSES
    )
    
    test_dataset = create_dataset(test_images, test_labels, shuffle=False)
    test_loss, test_accuracy = model.evaluate(test_dataset, verbose=0)
    
    print(f"\nğŸ“Š æµ‹è¯•é›†æ€§èƒ½:")
    print(f"   æµ‹è¯•æŸå¤±: {test_loss:.4f}")
    print(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")
    
    return test_accuracy

# b) å¢åŠ æ¨¡å‹æ€§èƒ½åˆ†æ
from sklearn.metrics import classification_report, confusion_matrix

def detailed_evaluation(model, test_df, num_samples=50):
    """è¯¦ç»†è¯„ä¼°æ¨¡å‹"""
    from sklearn.metrics import classification_report
    
    # éšæœºé€‰æ‹©ä¸€äº›æµ‹è¯•æ ·æœ¬
    sample_df = test_df.sample(min(num_samples, len(test_df)), random_state=42)
    
    predictions = []
    true_labels = []
    
    print(f"   æ­£åœ¨åˆ†æ {len(sample_df)} ä¸ªæ ·æœ¬...")
    
    for _, row in sample_df.iterrows():
        try:
            # åŠ è½½å’Œé¢„å¤„ç†å›¾ç‰‡
            img = tf.io.read_file(row['filepath'])
            img = tf.image.decode_jpeg(img, channels=3)
            img = tf.image.resize(img, IMG_SIZE)
            img = img / 255.0
            img = tf.expand_dims(img, axis=0)
            
            # é¢„æµ‹
            pred = model.predict(img, verbose=0)[0]
            pred_class = np.argmax(pred)
            
            predictions.append(pred_class)
            true_labels.append(row['class_id'])
            
        except Exception as e:
            continue
    
    if predictions:
        # è®¡ç®—å‡†ç¡®ç‡
        correct = sum(1 for p, t in zip(predictions, true_labels) if p == t)
        accuracy = correct / len(predictions)
        
        print(f"   Top-1å‡†ç¡®ç‡: {accuracy:.2%}")
        print(f"   æ­£ç¡®/æ€»æ•°: {correct}/{len(predictions)}")
        
        # æ‰“å°åˆ†ç±»æŠ¥å‘Šï¼ˆç®€å•ç‰ˆï¼‰
        unique_classes = set(true_labels)
        if len(unique_classes) <= 10:  # åªæ˜¾ç¤ºå°‘é‡ç±»åˆ«çš„æŠ¥å‘Š
            print("\n   åˆ†ç±»æŠ¥å‘Š:")
            print(classification_report(true_labels, predictions, digits=3))



if __name__ == "__main__":
    # å…ˆæµ‹è¯•æ•°æ®åŠ è½½
    if test_data_loading():
        # æ•°æ®åŠ è½½æ­£å¸¸ï¼Œå¼€å§‹è®­ç»ƒ
        main()
    else:
        print("\nâš ï¸  æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥:")
        print(f"   1. ç¡®ä¿ '{DATA_PATH}' æ–‡ä»¶å¤¹å­˜åœ¨")
        print(f"   2. ç¡®ä¿ '{IMAGES_DIR}' ä¸­æœ‰å›¾ç‰‡æ–‡ä»¶")
        print(f"   3. ç¡®ä¿ '{DATA_PATH}' ä¸­æœ‰ train.txt, val.txt, test.txt æ–‡ä»¶")