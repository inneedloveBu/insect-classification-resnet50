"""
IP102æ˜†è™«åˆ†ç±» - æœ€ç»ˆä¼˜åŒ–è®­ç»ƒè„šæœ¬
è¿è¡Œ: python train_advanced3.py
"""

import os
import math
import numpy as np
import pandas as pd
import tensorflow as tf
from datetime import datetime
from sklearn.utils.class_weight import compute_class_weight
# æ¨èï¼šç›´æ¥ä» tensorflow.keras.applications å¯¼å…¥
from tensorflow.keras.applications import ResNet50V2
from tensorflow.keras.applications.resnet_v2 import preprocess_input

# ==================== é…ç½®å‚æ•° ====================
DATA_PATH = 'ip102_v1.1'                # æ•°æ®é›†æ ¹ç›®å½•
IMAGES_DIR = os.path.join(DATA_PATH, 'images')
IMG_SIZE = (224, 224)                   # è¾“å…¥å›¾åƒå°ºå¯¸
BATCH_SIZE = 32                          # æ‰¹æ¬¡å¤§å°ï¼ˆæ ¹æ®GPUæ˜¾å­˜è°ƒæ•´ï¼‰
NUM_CLASSES = 102                        # æ˜†è™«ç±»åˆ«æ•°
USE_FULL_VAL = True                       # æ˜¯å¦ä½¿ç”¨å®Œæ•´éªŒè¯é›†ï¼ˆTrue å»ºè®®ï¼‰
EPOCHS_PHASE1 = 20                        # ç¬¬ä¸€é˜¶æ®µè®­ç»ƒè½®æ•°ï¼ˆå†»ç»“åŸºç¡€å±‚ï¼‰

LR_PHASE1 = 5e-5  #1e-4                          # ç¬¬ä¸€é˜¶æ®µå­¦ä¹ ç‡
USE_LABEL_SMOOTHING = True                 # æ˜¯å¦ä½¿ç”¨æ ‡ç­¾å¹³æ»‘
LABEL_SMOOTHING = 0.1                      # æ ‡ç­¾å¹³æ»‘å› å­
SAMPLE_SIZE = None  # ä½¿ç”¨å…¨éƒ¨æ•°æ®
EPOCHS_PHASE2 = 30  # å¯é€‚å½“å¢åŠ 

LR_PHASE2 = 2e-5    # æé«˜ä¸€ç‚¹   # ç¬¬äºŒé˜¶æ®µå­¦ä¹ ç‡

# ==================== æ•°æ®åŠ è½½ ====================

def check_annotation_file(filename):
    filepath = os.path.join(DATA_PATH, filename)
    with open(filepath, 'r') as f:
        for i, line in enumerate(f, 1):
            parts = line.strip().split()
            if len(parts) != 2:
                print(f"è¡Œ {i}: æ ¼å¼é”™è¯¯ -> {line.strip()}")
            else:
                try:
                    label = int(parts[1])
                    if label < 1 or label > 102:
                        print(f"è¡Œ {i}: æ ‡ç­¾è¶…å‡ºèŒƒå›´ {label} -> {line.strip()}")
                except:
                    print(f"è¡Œ {i}: æ ‡ç­¾éæ•°å­— -> {line.strip()}")

def load_annotation_file(filename):
    """åŠ è½½æ ‡æ³¨æ–‡ä»¶ï¼Œè¿”å›åŒ…å« 'filename', 'class_id', 'filepath' çš„ DataFrame"""
    filepath = os.path.join(DATA_PATH, filename)
    data = []

    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:  # è·³è¿‡ç©ºè¡Œ
                continue
            parts = line.split()
            if len(parts) >= 2:
                fname = parts[0]
                try:
                    class_id = int(parts[1]) - 1
                except:
                    print(f"è­¦å‘Š: æ ‡ç­¾è§£æå¤±è´¥: {line}")
                    continue
                # æ£€æŸ¥æ ‡ç­¾èŒƒå›´
                if class_id < 0 or class_id >= NUM_CLASSES:
                    print(f"è­¦å‘Š: æ ‡ç­¾ {class_id+1} è¶…å‡ºèŒƒå›´ï¼Œè·³è¿‡è¯¥è¡Œ: {line}")
                    continue
                full_path = os.path.join(IMAGES_DIR, fname)
                if os.path.exists(full_path):
                    data.append({
                        'filename': fname,
                        'class_id': class_id,
                        'filepath': full_path
                    })
    return pd.DataFrame(data)

# ==================== æ•°æ®é¢„å¤„ç†ä¸å¢å¼º ====================
def load_and_preprocess_image(filepath, label):
    """è¯»å–ã€è§£ç ã€è°ƒæ•´å¤§å°ï¼Œå¹¶ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¦æ±‚çš„é¢„å¤„ç†"""
    img = tf.io.read_file(filepath)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, IMG_SIZE)
    img = tf.cast(img, tf.float32)
    img = preprocess_input(img)               # å…³é”®ï¼šä½¿ç”¨ ResNet50V2 çš„é¢„å¤„ç†
    return img, label

def create_data_augmentation():
    """è®­ç»ƒæ•°æ®å¢å¼ºç­–ç•¥"""
    return tf.keras.Sequential([
        tf.keras.layers.RandomFlip("horizontal"),
        tf.keras.layers.RandomRotation(0.15),
        tf.keras.layers.RandomZoom(0.15),
        tf.keras.layers.RandomContrast(0.2),
        tf.keras.layers.RandomBrightness(0.1),
    ])

def create_dataset(image_paths, labels, shuffle=True, augment=False):
    """åˆ›å»º tf.data.Datasetï¼Œæ”¯æŒæ•°æ®å¢å¼º"""
    ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))
    ds = ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
    if shuffle:
        ds = ds.shuffle(buffer_size=min(len(image_paths), 5000))
    if augment:
        aug = create_data_augmentation()
        ds = ds.map(lambda x, y: (aug(x, training=True), y),
                    num_parallel_calls=tf.data.AUTOTUNE)
    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    return ds

# ==================== æ¨¡å‹æ„å»º ====================
# def create_advanced_model():
#     """æ„å»ºæ”¹è¿›çš„æ¨¡å‹ï¼ˆResNet50V2 + è‡ªå®šä¹‰åˆ†ç±»å¤´ï¼‰"""
#     base_model = ResNet50V2(
#         include_top=False,
#         weights='imagenet',
#         input_shape=(224, 224, 3),
#         pooling='avg'                     # å…¨å±€å¹³å‡æ± åŒ–
#     )
#     base_model.trainable = False           # ç¬¬ä¸€é˜¶æ®µå†»ç»“

#     # åœ¨ create_advanced_model() ä¸­ï¼ŒåŠ è½½ base_model å
#     print("Base model weights mean:", np.mean(base_model.get_weights()[0]))

#     inputs = tf.keras.Input(shape=(224, 224, 3))
#     x = base_model(inputs, training=False)
#     x = tf.keras.layers.Dropout(0.5)(x)
#     x = tf.keras.layers.Dense(1024, activation='relu')(x)
#     x = tf.keras.layers.BatchNormalization()(x)
#     x = tf.keras.layers.Dropout(0.5)(x)
#     x = tf.keras.layers.Dense(512, activation='relu')(x)
#     x = tf.keras.layers.BatchNormalization()(x)
#     x = tf.keras.layers.Dropout(0.3)(x)
#     outputs = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(x)

#     model = tf.keras.Model(inputs, outputs)
#     return model, base_model




def create_advanced_model():
    """æ„å»ºæ”¹è¿›çš„æ¨¡å‹ï¼ˆç®€åŒ–åˆ†ç±»å¤´ï¼‰"""
    base_model = ResNet50V2(
        include_top=False,
        weights='imagenet',
        input_shape=(224, 224, 3),
        pooling='avg'
    )
    base_model.trainable = False

    inputs = tf.keras.Input(shape=(224, 224, 3))
    x = base_model(inputs, training=False)
    x = tf.keras.layers.Dropout(0.5)(x)
    x = tf.keras.layers.Dense(512, activation='relu')(x)  # åªä¿ç•™ä¸€å±‚å…¨è¿æ¥
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Dropout(0.5)(x)
    outputs = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(x)

    model = tf.keras.Model(inputs, outputs)
    return model, base_model




# ==================== è®­ç»ƒæ›²çº¿ç»˜åˆ¶ ====================
def plot_training_history(history, save_dir):
    """ç»˜åˆ¶å¹¶ä¿å­˜è®­ç»ƒæ›²çº¿"""
    try:
        import matplotlib.pyplot as plt
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        # å‡†ç¡®ç‡
        ax1.plot(history['accuracy'], 'b-o', label='Training', markersize=4)
        ax1.plot(history['val_accuracy'], 'r-s', label='Validation', markersize=4)
        ax1.set_title('Model Accuracy')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Accuracy')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        # æŸå¤±
        ax2.plot(history['loss'], 'b-o', label='Training', markersize=4)
        ax2.plot(history['val_loss'], 'r-s', label='Validation', markersize=4)
        ax2.set_title('Model Loss')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Loss')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        plt.tight_layout()
        plot_path = os.path.join(save_dir, 'training_history.png')
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        print(f"   è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³: {plot_path}")
        plt.close()
    except Exception as e:
        print(f"   ç»˜åˆ¶æ›²çº¿å¤±è´¥: {e}")

# ==================== ä¸»è®­ç»ƒæµç¨‹ ====================
def main():

    print("æ£€æŸ¥è®­ç»ƒé›†...")
    check_annotation_file('train.txt')
    print("æ£€æŸ¥éªŒè¯é›†...")
    check_annotation_file('val.txt')
    print("=" * 70)
    print("ğŸ‹ï¸  IP102 æ˜†è™«åˆ†ç±» - ç»ˆæä¼˜åŒ–è®­ç»ƒ")
    print(f"   è®­ç»ƒé›†é‡‡æ ·: {SAMPLE_SIZE if SAMPLE_SIZE else 'å…¨éƒ¨'} å¼ ")
    print(f"   éªŒè¯é›†: {'å®Œæ•´' if USE_FULL_VAL else 'é‡‡æ ·'}")
    print(f"   ç¬¬ä¸€é˜¶æ®µè½®æ•°: {EPOCHS_PHASE1}, å­¦ä¹ ç‡: {LR_PHASE1}")
    print(f"   ç¬¬äºŒé˜¶æ®µè½®æ•°: {EPOCHS_PHASE2}, å­¦ä¹ ç‡: {LR_PHASE2}")
    print("=" * 70)

    # 1. åˆ›å»ºä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = f"training_advanced_{timestamp}"
    model_dir = os.path.join(save_dir, "models")
    os.makedirs(model_dir, exist_ok=True)
    print(f"âœ… ä¿å­˜ç›®å½•: {save_dir}")

    # 2. åŠ è½½æ•°æ®
    print("\n[1/5] åŠ è½½æ•°æ®...")
    try:
        train_df = load_annotation_file('train.txt')
        val_df = load_annotation_file('val.txt')
        print(f"   å®Œæ•´è®­ç»ƒé›†: {len(train_df):,} å¼ ")
        print(f"   å®Œæ•´éªŒè¯é›†: {len(val_df):,} å¼ ")

        # è®­ç»ƒé›†é‡‡æ ·ï¼ˆå¯é€‰ï¼‰
        # if SAMPLE_SIZE and len(train_df) > SAMPLE_SIZE:
        #     # å°è¯•ä¿æŒç±»åˆ«å¹³è¡¡çš„é‡‡æ ·
        #     samples_per_class = SAMPLE_SIZE // NUM_CLASSES
        #     sampled = []
        #     for cls in range(NUM_CLASSES):
        #         cls_df = train_df[train_df['class_id'] == cls]

        #         print("Class ID range:", train_df['class_id'].min(), "-", train_df['class_id'].max())


        #         if len(cls_df) > 0:
        #             n = min(len(cls_df), max(1, samples_per_class))
        #             sampled.append(cls_df.sample(n, random_state=42))
        #     train_df = pd.concat(sampled, ignore_index=True)
        #     print(f"   é‡‡æ ·åè®­ç»ƒé›†: {len(train_df):,} å¼  (å¹³è¡¡é‡‡æ ·)")

        # éªŒè¯é›†å¤„ç†
        if not USE_FULL_VAL and len(val_df) > 3000:
            val_df = val_df.sample(3000, random_state=42)
        print(f"   æœ€ç»ˆéªŒè¯é›†: {len(val_df):,} å¼ ")

    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return

    # 3. è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆå¤„ç†ä¸å¹³è¡¡ï¼‰
    print("\n[2/5] è®¡ç®—ç±»åˆ«æƒé‡...")
    classes = train_df['class_id'].values
    class_weights = compute_class_weight('balanced',
                                         classes=np.unique(classes),
                                         y=classes)
    class_weight_dict = dict(enumerate(class_weights))
    print(f"   ç±»åˆ«æƒé‡èŒƒå›´: [{min(class_weights):.3f}, {max(class_weights):.3f}]")

    # 4. å‡†å¤‡æ•°æ®ç®¡é“
    print("\n[3/5] å‡†å¤‡æ•°æ®ç®¡é“...")
    train_images = train_df['filepath'].values
    train_labels = tf.keras.utils.to_categorical(train_df['class_id'].values, NUM_CLASSES)
    val_images = val_df['filepath'].values
    val_labels = tf.keras.utils.to_categorical(val_df['class_id'].values, NUM_CLASSES)

    train_dataset = create_dataset(train_images, train_labels,
                                   shuffle=True, augment=False)# ##################################################
    val_dataset = create_dataset(val_images, val_labels,
                                 shuffle=False, augment=False)
    

    # åœ¨åˆ›å»ºæ•°æ®é›†åï¼Œç«‹å³æ£€æŸ¥ä¸€ä¸ª batch
    for images, labels in train_dataset.take(1):
        print("Image batch shape:", images.shape)
        print("Label batch shape:", labels.shape)
        print("Labels sample:", labels[0].numpy())  # åº”è¯¥æ˜¯ one-hot å‘é‡
        # å¯ä»¥æ˜¾ç¤ºä¸€å¼ å›¾ï¼ˆå¯é€‰ï¼‰
        import matplotlib.pyplot as plt
        plt.imshow(images[0].numpy() * 0.5 + 0.5)  # å› ä¸º preprocess_input å°†åƒç´ èŒƒå›´å˜ä¸º [-1,1] å·¦å³ï¼Œéœ€è¦åæ ‡å‡†åŒ–æ‰èƒ½æ­£ç¡®æ˜¾ç¤º
        plt.title("Sample image")
        plt.show()










    # 5. æ„å»ºæ¨¡å‹
    print("\n[4/5] æ„å»ºæ¨¡å‹...")
    model, base_model = create_advanced_model()
    model.summary()

    # 6. è®¾ç½®å›è°ƒ
    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(
            filepath=os.path.join(model_dir, 'best_model.h5'),
            monitor='val_accuracy',
            save_best_only=True,
            mode='max',
            verbose=1
        ),
        tf.keras.callbacks.CSVLogger(
            filename=os.path.join(save_dir, 'training_log.csv')
        ),
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=4,
            min_lr=1e-7,
            verbose=1
        )
    ]

    # 7. ç¬¬ä¸€é˜¶æ®µè®­ç»ƒï¼ˆå†»ç»“åŸºç¡€å±‚ï¼‰
    print("\n[5/5] ç¬¬ä¸€é˜¶æ®µè®­ç»ƒ (å†»ç»“åŸºç¡€å±‚)...")
    loss_fn = (tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)
               if USE_LABEL_SMOOTHING else 'categorical_crossentropy')
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=LR_PHASE1),
        loss=loss_fn,
        metrics=['accuracy']
    )

    history1 = model.fit(
        train_dataset,
        validation_data=val_dataset,
        # epochs=EPOCHS_PHASE1,
        epochs=10,
        # class_weight=class_weight_dict,###################################################
        callbacks=callbacks,
        verbose=1
    )

    # 8. ç¬¬äºŒé˜¶æ®µå¾®è°ƒ
    print("\nğŸ”§ ç¬¬äºŒé˜¶æ®µï¼šå¾®è°ƒéƒ¨åˆ†å±‚")
    base_model.trainable = True
    # åªå¾®è°ƒæœ€å30å±‚
    for layer in base_model.layers[:-30]:
        layer.trainable = False

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=LR_PHASE2),
        loss=loss_fn,
        metrics=['accuracy']
    )

    history2 = model.fit(
        train_dataset,
        validation_data=val_dataset,
        epochs=EPOCHS_PHASE2,
        class_weight=class_weight_dict,
        callbacks=callbacks,
        verbose=1
    )

    # 9. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = os.path.join(model_dir, 'final_model.h5')
    model.save(final_model_path)
    print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")

    # 10. è¾“å‡ºç»“æœä¸ç»˜å›¾
    print("\n" + "=" * 70)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("=" * 70)

    # åˆå¹¶è®­ç»ƒå†å²
    history = {
        'accuracy': history1.history['accuracy'] + history2.history['accuracy'],
        'val_accuracy': history1.history['val_accuracy'] + history2.history['val_accuracy'],
        'loss': history1.history['loss'] + history2.history['loss'],
        'val_loss': history1.history['val_loss'] + history2.history['val_loss']
    }
    final_val_acc = history['val_accuracy'][-1]
    best_val_acc = max(history['val_accuracy'])

    print(f"   æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_val_acc:.2%}")
    print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2%}")
    print(f"   æ¨¡å‹ä¿å­˜ä½ç½®: {model_dir}/")

    plot_training_history(history, save_dir)
    print(f"âœ… æ‰€æœ‰è¾“å‡ºä¿å­˜åœ¨: {save_dir}")

if __name__ == "__main__":
    # è®¾ç½® TensorFlow æ—¥å¿—çº§åˆ«ï¼ˆå¯é€‰ï¼‰
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    main()